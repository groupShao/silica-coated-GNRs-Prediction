import pandas as pd
import xgboost as xgb
import sklearn
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier,plot_importance
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error,accuracy_score,roc_auc_score
from sklearn.model_selection import RepeatedKFold,StratifiedKFold,GridSearchCV
from sklearn import metrics
import numpy as np


data = pd.read_csv('Au_SiO2_data.csv')
data_clear=data.dropna(axis=0, how='any', subset=['Shell'])
X = data_clear.copy()
X["TEOS Solvent encoded"] = X.groupby("TEOS Solvent")["Shell"].transform("mean")
X=X.drop("TEOS Solvent",axis=1)
target=X.pop("Shell")
X_train, X_test, y_train, y_test = train_test_split(X, target,test_size=0.2)


##XGBoost
param_grid_xgb = {
        'max_depth':list(range(3,10)),
        'learning_rate':[0.01,0.02,0.05,0.08,0.1,0.2,0.3],
        'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],
        'subsample':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0],
        'colsample_bytree':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0],
        'reg_alpha': [0.05, 0.1, 1, 2, 3], 
        'reg_lambda': [0.05, 0.1, 1, 2, 3],
        'min_child_weight':list(range(0,9))
        }
xgb = xgb.XGBClassifier(max_depth=5,
                        learning_rate=0.1,
                        n_estimators=400,
                        objective='binary:logistic',
                        gamma=0,
                        min_child_weight=1,
                        max_delta_step=0,
                        subsample=0.6,
                        colsample_bytree=0.7,
                        colsample_bylevel=1,
                        reg_alpha=0,
                        reg_lambda=1,
                        scale_pos_weight=1,
                        seed=2023)
xgb_grid_search = GridSearchCV(xgb, param_grid_xgb, cv=5,
                          scoring='accuracy')
xgb_grid_search.fit(X_train,y_train)
xgb_best_parameters = xgb_grid_search.best_params_
xgb = xgb.XGBClassifier(**xgb_best_parameters)
xgb.fit(X_train,y_train)

test_score = xgb.score(X_test,y_test)
print('Best parameters:{}'.format(xgb_best_parameters))
print('Best score on test set:{:.2f}'.format(test_score))

##XGBoost modeling building
dtrain=xgb.DMatrix(train_x,label=train_y)
dtest=xgb.DMatrix(test_x,label=test_y)
params={'booster':'gbtree',
        'objective': 'binary:logistic',
        'eval_metric': 'error',
        'min_child_weight':1,
        'max_depth':5,
        'lambda':0,
        'alpha':0,
        'gamma': 0.4,
        'subsample':0.9,
        'colsample_bytree':0.9,
        'colsample_bylevel':1,
        'max_delta_step':0,    
        'eta': 0.01,
        'seed':0,
        'nthread':8,
        'scale_pos_weight':1
       }
res = xgb.cv(params,dtrain,num_boost_round=2000,metrics='logloss',early_stopping_rounds=25,nfold=5,seed=0)
best_nround = res.shape[0] - 1

watchlist = [(dtrain,'train'),(dtest,'test')]
bst=xgb.train(params,dtrain,num_boost_round=best_nround,evals=watchlist)
ypred=bst.predict(dtest)
ypred2=bst.predict(dtrain)
y_pred = (ypred >= 0.5)*1

print ('AUC(train): %.4f' % metrics.roc_auc_score(train_y,ypred2))
print('best_nround:',best_nround)
print ('AUC: %.4f' % metrics.roc_auc_score(test_y,ypred))
print ('ACC: %.4f' % metrics.accuracy_score(test_y,y_pred))
print ('Recall: %.4f' % metrics.recall_score(test_y,y_pred))
print ('F1-score: %.4f' %metrics.f1_score(test_y,y_pred))
print ('Precesion: %.4f' %metrics.precision_score(test_y,y_pred))
print('Log_loss: %f'%metrics.log_loss(test_y,y_pred))
print(metrics.confusion_matrix(test_y,y_pred))
